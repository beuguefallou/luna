# Google AI Hackathon - Abidjan 2025
##  Luna (apaisement, comme la lune) - Chatbot pour la sant√© mentale
Assistant de soutien √† l'empathie, au r√©confort et au soutien √©motionnel

## üß† √ânonc√© du probl√®me
Lors des √©lections pr√©sidentielles du S√©n√©gal en 2024, plus de 2000 personnes ont √©t√© d√©tenues dans un contexte fortement marqu√© par des tensions politiques, de l'incertitude et des divisions sociales. Ce climat a profond√©ment affect√© la sant√© mentale de nombreuses personnes, qu‚Äôelles soient directement concern√©es par les d√©tentions ou t√©moins de ces √©v√©nements.

Plus largement, des millions de personnes dans le monde souffrent √©galement de d√©pression, d'anxi√©t√© et de solitude. Beaucoup n‚Äôont personne √† qui parler, et l‚Äôabsence de soutien psychologique peut entra√Æner des cons√©quences graves, voire irr√©versibles.

Luna vise √† r√©pondre √† cette urgence en offrant un espace s√ªr, anonyme et bienveillant o√π chacun peut exprimer ses pens√©es et recevoir des r√©ponses empathiques, motivantes et sans jugement, 24h/24 et 7j/7.

## üí° Motivation et objectif
R√©ponse √† une crise r√©elle : L'id√©e de Luna est n√©e en constatant l‚Äôimpact psychologique des d√©tentions massives li√©es aux √©lections de 2024 au S√©n√©gal. Une crise silencieuse qui a mis en lumi√®re l‚Äôurgence d‚Äôoutils de soutien accessibles √† tous.

- **Empathie √† grande √©chelle** : Offrir un acc√®s universel au soutien en mati√®re de sant√© mentale, en particulier dans les contextes de crise politique ou sociale.

- **Confidentialit√© avant tout** : Aucun compte requis, aucun suivi. L‚Äôutilisateur reste totalement anonyme.

- **Impact positif durable** : Encourager, apaiser et accompagner les utilisateurs avec douceur, respect et humanit√©.

---

## üõ†Ô∏è Pile technologique

- **Frontend:** React, TypeScript, Tailwind CSS
- **Backend:** Next.js (Node.js), SQLite
- **Model AI:** Local LLMs (Gemma via Ollama)
- **Deploiement:** Docker, Vercel/Netlify (frontend), Render/Heroku (backend)

---

## üöÄ Fonctionnalit√©s

- Discussion anonyme et encourageante avec un compagnon de sant√© mentale IA
- Interface utilisateur moderne et apaisante ax√©e sur le confort de l'utilisateur
- R√©ponses motivationnelles, positives et empathiques
- Suggestions d'articles, de m√©ditation et de groupes de soutien
- Historique de discussion persistant et prise en charge multi-chat
- Inf√©rence IA rapide et locale (aucune API cloud requise)

---

## üñ•Ô∏è  Comment ex√©cuter localement

### **Exigences**
- Node.js (v18+ recommand√©)
- npm
- [Ollama](https://ollama.com/) (for local LLMs)
- Git

### **√âtapes de configuration**

1. **Cloner le d√©p√¥t:**
   ```sh
   git clone https://gitlab.com/beuguefallou/luna.git
   cd luna
   ```

2. **Installer les d√©pendances:**
   ```sh
   # Install backend dependencies
   cd backend
   npm install
   
   # Install frontend dependencies
   cd ../frontend
   npm install
   ```

3. **Configurer Ollama:**
   ```sh
   # Install Ollama from https://ollama.com/
   # Pull the Gemma model
   ollama pull gemma3
   ```

4. **Initialiser la base de donn√©es:**
   ```sh
   cd backend
   npm run build
   ```

### **Ex√©cution de l'application**

1. **D√©marrer Ollama (Port : 11434)**
   ```sh
   ollama serve
   ```
   - Cela d√©marrera le serveur Ollama √† http://localhost:11434
   - Gardez cette fen√™tre de terminal ouverte

2. **D√©marrer le backend (Port : 3001)**
   ```sh
   cd backend
    npm run dev
   ```
   or
   ```bash
         cd backend
         npm run dev
   ```
         or
   ```cmd
         cd backend
         npm run dev

   ```


   - Cela d√©marrera l'application React √† http://localhost:3001
   - Gardez cette fen√™tre de terminal ouverte

3. **D√©marrer le frontend (Port : 3000)**
   ```sh
   cd frontend
   npm start
   ```
   - Cela d√©marrera l'application React √† http://localhost:3000
   - L'application s'ouvrira automatiquement dans votre navigateur par d√©faut

# Autres M√©thodes
## Option 1 : Utiliser Windows Terminal
Sur Windows, tu peux utiliser Windows Terminal (si tu ne l'as pas encore install√©, tu peux le t√©l√©charger sur le Microsoft Store).

Pour utiliser Windows Terminal dans ton script, tu peux essayer quelque chose comme ceci dans run.sh :

```bash
start wt -w 0 new-tab -d . bash -c "votre_commande; exec bash"
```
wt est l'ex√©cutable de Windows Terminal.

- ``-w 0`` sp√©cifie que tu veux ouvrir le terminal dans la fen√™tre par d√©faut.

new-tab ouvre un nouvel onglet dans Windows Terminal.

- ``-d .`` sp√©cifie le r√©pertoire de travail (le r√©pertoire courant dans ce cas).

- ``bash -c "votre_commande; exec bash"`` ex√©cute une commande et garde la fen√™tre ouverte.

## Option 2 : Utiliser cmd ou PowerShell
Si tu ne veux pas utiliser Windows Terminal, tu peux utiliser cmd ou PowerShell pour ouvrir des nouvelles fen√™tres de terminal. Exemple :

CMD :

```bash
start cmd /k "votre_commande"
```
PowerShell :

```bash

start powershell -NoExit -Command "votre_commande"
```
Cela ouvrira une nouvelle fen√™tre de terminal et ex√©cutera la commande sp√©cifi√©e.
## üîß Option 3 : Installer gnome-terminal
Si vous utilisez un syst√®me bas√© sur GNOME (comme Ubuntu), vous pouvez simplement l‚Äôinstaller avec :

```bash
sudo apt update
sudo apt install gnome-terminal
```


## ‚úÖ 1. Solution universelle (Windows + PowerShell) avec un .ps1
Tu peux cr√©er un script PowerShell (run.ps1) comme ceci :
```
powershell

# run.ps1

# Lancement de l'installation dans PowerShell
Start-Process powershell -ArgumentList "cd backend; npm install" -NoNewWindow
Start-Process powershell -ArgumentList "cd frontend; npm install" -NoNewWindow

# Lancement du t√©l√©chargement du mod√®le Gemma
Start-Process powershell -ArgumentList "ollama pull gemma3"

# Compilation backend
Start-Process powershell -ArgumentList "cd backend; npm run build"

# D√©marrage de Ollama (Port : 11434)
Start-Process powershell -ArgumentList "ollama serve"

# D√©marrage du backend (Port : 3001)
Start-Process powershell -ArgumentList "cd backend; $env:PORT=3001; npm run dev"

# D√©marrage du frontend (Port : 3000)
Start-Process powershell -ArgumentList "cd frontend; npm start"
```
> ‚ö†Ô∏è Pour ex√©cuter ce fichier :
Ouvre PowerShell en mode administrateur et ex√©cute :
```
powershell

Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
./run.ps1
```
## ‚úÖ 2. Solution CMD / .bat pour Windows uniquement
Cr√©e un fichier run.bat :
```
bat

@echo off

start "Install backend dependencies" cmd /k "cd backend && npm install"
start "Install frontend dependencies" cmd /k "cd frontend && npm install"
start "Pull gemma model" cmd /k "ollama pull gemma3"
start "Build backend" cmd /k "cd backend && npm run build"
start "Start ollama" cmd /k "ollama serve"
start "Start backend" cmd /k "cd backend && set PORT=3001 && npm run dev"
start "Start frontend" cmd /k "cd frontend && npm start"
Double-clique sur le fichier pour tout lancer dans des fen√™tres de terminal s√©par√©es.
```

## ‚úÖ 3. Script Bash (Linux/macOS ou Git Bash sur Windows)
Cr√©e un fichier run.sh :
```
bash

#!/bin/bash

# Install backend deps
gnome-terminal -- bash -c "cd backend && npm install; exec bash"
# Install frontend deps
gnome-terminal -- bash -c "cd frontend && npm install; exec bash"

# Pull gemma model
gnome-terminal -- bash -c "ollama pull gemma3; exec bash"

# Build backend
gnome-terminal -- bash -c "cd backend && npm run build; exec bash"

# Start ollama
gnome-terminal -- bash -c "ollama serve; exec bash"

# Start backend
gnome-terminal -- bash -c "cd backend && PORT=3001 npm run dev; exec bash"

# Start frontend
gnome-terminal -- bash -c "cd frontend && npm start; exec bash"
Si tu es sous WSL ou Ubuntu, installe gnome-terminal ou adapte avec xterm, konsole, etc.
```
> üß† Recommandation
- Si tu es sous Windows uniquement, utilise le .bat ou .ps1

- Si tu es sur Git Bash ou WSL, pr√©f√®re le run.sh



### **Configuration du port**
- Frontend: http://localhost:3000
- Backend: http://localhost:3001
- Ollama: http://localhost:11434

### **D√©pannage**
Si vous rencontrez des conflits de ports :
1. V√©rifiez si un processus utilise le port 3000 :

   ```sh
   netstat -ano | findstr :3000
   ```
2. Tuez le processus si n√©cessaire :
   ```sh
   taskkill /F /PID <process_id>
   ```

 Ollama affiche  "port d√©j√† utilis√© ":
1. V√©rifiez si Ollama est d√©j√† en cours d'ex√©cution
2. Tuer tous les processus Ollama existants
3. Red√©marrer Ollama

---

## üìù Am√©liorations futures

- Historique de discussion persistant et prise en charge multi-chat
- Des suggestions et des ressources plus personnalis√©es
- Interface utilisateur adapt√©e aux mobiles
- Options de d√©ploiement dans le cloud


---

## ü§ù Contributions
Les pull requests et suggestions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue pour discuter de vos id√©es.

---

## üìÑ License

MIT

---

## üôè Remerciements

- [Ollama](https://ollama.com/)
- [Gemma LLM](https://ai.google.dev/gemma)
- [React](https://react.dev/)
- [Next.js](https://nextjs.org/)
- [Tailwind CSS](https://tailwindcss.com/)

---
## Quelques fonctionnalit√©s de l'application:-


Page d'accueil                 |                   Reponse ChatBot
:---------------------------------:        |      :------------------------------:
<img src="Screenshots/Capture1.PNG" height="200">  | <img src="Screenshots/Capture.PNG" height="200">


> "Si vous ou quelqu'un que vous connaissez √©prouvez des difficult√©s, veuillez contacter un professionnel de la sant√© mentale ou une ligne d'√©coute de votre pays. LUNA. ne remplace pas une aide professionnelle.